{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and init findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001A47C6E75C0>\n"
     ]
    }
   ],
   "source": [
    "# Import required library\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"Online Retail\").getOrCreate()\n",
    "\n",
    "# Print Spark object ID\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas to read excel\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset could be downloaded at https://www.kaggle.com/puneetbhaya/online-retail/\n",
    "datafile = pandas.read_excel(\"D://kuliah//bigdata//online_retail.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 541909 entries, 0 to 541908\n",
      "Data columns (total 8 columns):\n",
      "InvoiceNo      541909 non-null object\n",
      "StockCode      541909 non-null object\n",
      "Description    540455 non-null object\n",
      "Quantity       541909 non-null int64\n",
      "InvoiceDate    541909 non-null datetime64[ns]\n",
      "UnitPrice      541909 non-null float64\n",
      "CustomerID     406829 non-null float64\n",
      "Country        541909 non-null object\n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(4)\n",
      "memory usage: 33.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Show detail of input data\n",
    "datafile.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from input\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "dataSchema = StructType([ StructField(\"InvoiceNo\", StringType(), True)\\\n",
    "                       ,StructField(\"StockCode\", StringType(), True)\\\n",
    "                       ,StructField(\"Description\", StringType(), True)\\\n",
    "                       ,StructField(\"Quantity\", IntegerType(), True)\\\n",
    "                       ,StructField(\"InvoiceDate\", TimestampType(), True)\\\n",
    "                       ,StructField(\"UnitPrice\", FloatType(), True)\\\n",
    "                       ,StructField(\"CustomerID\", FloatType(), True)\\\n",
    "                       ,StructField(\"Country\", StringType(), True)])\n",
    "\n",
    "df = spark.createDataFrame(datafile, schema = dataSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: float (nullable = true)\n",
      " |-- CustomerID: float (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Shows detail of dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 01:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 01:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 01:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 01:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 01:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 01:26:00|     7.65|   17850.0|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 01:26:00|     4.25|   17850.0|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 01:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 01:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 01:34:00|     1.69|   13047.0|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|2010-12-01 01:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|2010-12-01 01:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|2010-12-01 01:34:00|     3.75|   13047.0|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|2010-12-01 01:34:00|     1.65|   13047.0|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|2010-12-01 01:34:00|     4.25|   13047.0|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|2010-12-01 01:34:00|     4.95|   13047.0|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|2010-12-01 01:34:00|     9.95|   13047.0|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|2010-12-01 01:34:00|     5.95|   13047.0|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|2010-12-01 01:34:00|     5.95|   13047.0|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|2010-12-01 01:34:00|     7.95|   13047.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print top 20 rows data\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ID and Items column for processing\n",
    "data = df.selectExpr(['InvoiceNo as ID', 'StockCode as ITEMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|    ID| ITEMS|\n",
      "+------+------+\n",
      "|536365|85123A|\n",
      "|536365| 71053|\n",
      "|536365|84406B|\n",
      "|536365|84029G|\n",
      "|536365|84029E|\n",
      "|536365| 22752|\n",
      "|536365| 21730|\n",
      "|536366| 22633|\n",
      "|536366| 22632|\n",
      "|536367| 84879|\n",
      "|536367| 22745|\n",
      "|536367| 22748|\n",
      "|536367| 22749|\n",
      "|536367| 22310|\n",
      "|536367| 84969|\n",
      "|536367| 22623|\n",
      "|536367| 22622|\n",
      "|536367| 21754|\n",
      "|536367| 21755|\n",
      "|536367| 21777|\n",
      "+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show two colums data\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library for grouping\n",
    "from pyspark.sql.functions import collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group ITEMS from data by ID \n",
    "data_grouped = data.groupby(\"ID\").agg(collect_list('ITEMS').alias('ITEMS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|    ID|               ITEMS|\n",
      "+------+--------------------+\n",
      "|536596|[21624, 22900, 22...|\n",
      "|536938|[22386, 85099C, 2...|\n",
      "|537252|             [22197]|\n",
      "|537691|[22791, 22171, 82...|\n",
      "|538041|             [22145]|\n",
      "|538184|[22585, 21481, 22...|\n",
      "|538517|[22491, 21232, 21...|\n",
      "|538879|[84819, 22150, 21...|\n",
      "|539275|[22909, 22423, 22...|\n",
      "|539630|[21484, 85099B, 2...|\n",
      "|540499|[21868, 22697, 22...|\n",
      "|540540|[21877, 21868, 21...|\n",
      "|540976|[22394, 21890, 22...|\n",
      "|541432|[21485, 22457, 84...|\n",
      "|541518|[21880, 21881, 21...|\n",
      "|541783|[22423, 22854, 22...|\n",
      "|542026|[21754, 82600, 22...|\n",
      "|542375|[21731, 22367, 22...|\n",
      "|543641|[85123A, 21833, 2...|\n",
      "|544303|[22660, 48138, 48...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show grouped data\n",
    "data_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set distinct ITEMS from each ID\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "distinct_udf = udf(lambda row: list(set(row)), ArrayType(StringType()))\n",
    "data_grouped_distinct = data_grouped.withColumn(\"ITEMS\", distinct_udf(\"ITEMS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|    ID|               ITEMS|\n",
      "+------+--------------------+\n",
      "|536596|[84926A, 21624, 2...|\n",
      "|536938|[21479, 84997B, 2...|\n",
      "|537252|             [22197]|\n",
      "|537691|[20975, 22149, 21...|\n",
      "|538041|             [22145]|\n",
      "|538184|[22492, 22561, 48...|\n",
      "|538517|[22197, 22844, 22...|\n",
      "|538879|[22593, 22983, 22...|\n",
      "|539275|[22423, 21914, 22...|\n",
      "|539630|[22988, 84347, 22...|\n",
      "|540499|[21755, 84978, 22...|\n",
      "|540540|[22555, 22551, 22...|\n",
      "|540976|[22207, 21110, 84...|\n",
      "|541432|[22113, 22457, 21...|\n",
      "|541518|[20724, 21982, 20...|\n",
      "|541783|[22197, 84978, 22...|\n",
      "|542026|[22197, 22398, 22...|\n",
      "|542375|[22367, 22629, 21...|\n",
      "|543641|[22371, 44265, 21...|\n",
      "|544303|[20856, 22197, 20...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show grouped distinct data\n",
    "data_grouped_distinct.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library for calculating frequently co-occurring items\n",
    "from pyspark.ml.fpm import FPGrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate frequent itemsets with minimum support 0.4 and minimum confidence 0.8\n",
    "fpgrowth1 = FPGrowth(itemsCol=\"ITEMS\", minSupport=0.4, minConfidence=0.8)\n",
    "model1 = fpgrowth1.fit(data_grouped_distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|items|freq|\n",
      "+-----+----+\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "model1.freqItemsets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No result from calculation above, lower minimum support to 0.05 and minimum confidence to 0.7\n",
    "fpgrowth2 = FPGrowth(itemsCol=\"ITEMS\", minSupport=0.05, minConfidence=0.7)\n",
    "model2 = fpgrowth2.fit(data_grouped_distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|   items|freq|\n",
      "+--------+----+\n",
      "|[85123A]|2246|\n",
      "| [22423]|2172|\n",
      "|[85099B]|2135|\n",
      "| [47566]|1706|\n",
      "| [20725]|1608|\n",
      "| [84879]|1468|\n",
      "| [22720]|1462|\n",
      "| [22197]|1442|\n",
      "| [21212]|1334|\n",
      "| [22383]|1306|\n",
      "| [20727]|1295|\n",
      "+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "model2.freqItemsets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|antecedent|consequent|confidence|\n",
      "+----------+----------+----------+\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show association rules\n",
    "model2.associationRules.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower minimum support to 0.02 and minimum confidence to 0.6\n",
    "fpgrowth3 = FPGrowth(itemsCol=\"ITEMS\", minSupport=0.02, minConfidence=0.6)\n",
    "model3 = fpgrowth3.fit(data_grouped_distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+\n",
      "|          items|freq|\n",
      "+---------------+----+\n",
      "|       [85123A]|2246|\n",
      "|        [22423]|2172|\n",
      "|       [85099B]|2135|\n",
      "|        [47566]|1706|\n",
      "|        [20725]|1608|\n",
      "|[20725, 85099B]| 588|\n",
      "|        [84879]|1468|\n",
      "|        [22720]|1462|\n",
      "|        [22197]|1442|\n",
      "|        [21212]|1334|\n",
      "|        [22383]|1306|\n",
      "| [22383, 20725]| 663|\n",
      "|        [20727]|1295|\n",
      "| [20727, 20725]| 648|\n",
      "| [20727, 22383]| 587|\n",
      "|        [22457]|1266|\n",
      "|         [POST]|1254|\n",
      "|        [23203]|1249|\n",
      "|[23203, 85099B]| 582|\n",
      "|        [22386]|1231|\n",
      "+---------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "model3.freqItemsets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------------------+\n",
      "|    antecedent|consequent|        confidence|\n",
      "+--------------+----------+------------------+\n",
      "|       [22699]|   [22697]|               0.7|\n",
      "|       [22386]|  [85099B]|0.6766856214459789|\n",
      "|       [22910]|   [22086]|0.6670673076923077|\n",
      "|       [22630]|   [22629]|0.6255813953488372|\n",
      "|       [22726]|   [22727]|0.6440677966101694|\n",
      "|[22698, 22697]|   [22699]|0.8524844720496895|\n",
      "|       [21931]|  [85099B]|0.6103247293921732|\n",
      "|      [85099F]|  [85099B]|0.6566265060240963|\n",
      "|       [20712]|  [85099B]|0.6169724770642202|\n",
      "|       [22698]|   [22697]|0.8029925187032418|\n",
      "|       [22698]|   [22699]|0.7655860349127181|\n",
      "|       [22697]|   [22699]|0.7417218543046358|\n",
      "|       [22697]|   [22698]| 0.609271523178808|\n",
      "|       [21928]|  [85099B]|0.6691176470588235|\n",
      "|[22698, 22699]|   [22697]|0.8941368078175895|\n",
      "|      [85099C]|  [85099B]|0.6261879619852164|\n",
      "|[22697, 22699]|   [22698]|0.7002551020408163|\n",
      "|       [22356]|   [20724]|0.6921052631578948|\n",
      "|       [23300]|   [23301]|0.7176470588235294|\n",
      "+--------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show association rules\n",
    "model3.associationRules.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------+\n",
      "|    ID|               ITEMS|    prediction|\n",
      "+------+--------------------+--------------+\n",
      "|536596|[84926A, 21624, 2...|            []|\n",
      "|536938|[21479, 84997B, 2...|      [85099B]|\n",
      "|537252|             [22197]|            []|\n",
      "|537691|[20975, 22149, 21...|            []|\n",
      "|538041|             [22145]|            []|\n",
      "|538184|[22492, 22561, 48...|            []|\n",
      "|538517|[22197, 22844, 22...|            []|\n",
      "|538879|[22593, 22983, 22...|            []|\n",
      "|539275|[22423, 21914, 22...|            []|\n",
      "|539630|[22988, 84347, 22...|            []|\n",
      "|540499|[21755, 84978, 22...|[22698, 20724]|\n",
      "|540540|[22555, 22551, 22...|            []|\n",
      "|540976|[22207, 21110, 84...|            []|\n",
      "|541432|[22113, 22457, 21...|            []|\n",
      "|541518|[20724, 21982, 20...|            []|\n",
      "|541783|[22197, 84978, 22...|       [22698]|\n",
      "|542026|[22197, 22398, 22...|            []|\n",
      "|542375|[22367, 22629, 21...|            []|\n",
      "|543641|[22371, 44265, 21...|            []|\n",
      "|544303|[20856, 22197, 20...|            []|\n",
      "+------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform item data with association rules to get predictions\n",
    "model3.transform(data_grouped_distinct).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First test is to predict item 22698"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| ID|  ITEMS|\n",
      "+---+-------+\n",
      "|  0|[22698]|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set item 22698 into dataframe\n",
    "df_predict1 = spark.createDataFrame([('0',['22698'])],['ID','ITEMS'])\n",
    "df_predict1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+\n",
      "| ID|  ITEMS|    prediction|\n",
      "+---+-------+--------------+\n",
      "|  0|[22698]|[22697, 22699]|\n",
      "+---+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use model3 to make prediction\n",
    "model3.transform(df_predict1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PINK REGENCY TEACUP AND SAUCER'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the description of item that being predicted\n",
    "df.select('Description').filter(df.StockCode == '22698').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GREEN REGENCY TEACUP AND SAUCER'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the description of prediction item 22697\n",
    "df.select('Description').filter(df.StockCode == '22697').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ROSES REGENCY TEACUP AND SAUCER '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the description of prediction item 22699\n",
    "df.select('Description').filter(df.StockCode == '22699').collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Second test is to predict item 22386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| ID|  ITEMS|\n",
      "+---+-------+\n",
      "|  0|[22386]|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set item 22386 into dataframe\n",
    "df_predict2 = spark.createDataFrame([('0',['22386'])],['ID','ITEMS'])\n",
    "df_predict2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+\n",
      "| ID|  ITEMS|prediction|\n",
      "+---+-------+----------+\n",
      "|  0|[22386]|  [85099B]|\n",
      "+---+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use model3 to make prediction\n",
    "model3.transform(df_predict2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JUMBO BAG PINK POLKADOT'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the description of item that being predicted\n",
    "df.select('Description').filter(df.StockCode == '22386').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JUMBO BAG RED RETROSPOT'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the description of prediction item 85099B\n",
    "df.select('Description').filter(df.StockCode == '85099B').collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From predictions above we could conclude that:\n",
    "- Customers that buy item 22698 (PINK REGENCY TEACUP AND SAUCER) are most likely to buy item 22697 (GREEN REGENCY TEACUP AND SAUCER) and/or 22699 (ROSES REGENCY TEACUP AND SAUCER) too\n",
    "- Customers that buy item 22386 (JUMBO BAG PINK POLKADOT) are most likely to buy item 85099B (JUMBO BAG RED RETROSPOT) too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Excel using Python - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html\n",
    "- Remove duplicates from PySpark array column - https://stackoverflow.com/questions/54185710/remove-duplicates-from-pyspark-array-column\n",
    "- Convert spark DataFrame column to python list - https://stackoverflow.com/questions/38610559/convert-spark-dataframe-column-to-python-list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
